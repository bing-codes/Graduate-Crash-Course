{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 -- Python for Researchers\n",
    "\n",
    "## Today's Goals:\n",
    "   * Learn how to open and read .txt files\n",
    "   * Explore Python libraries, how to use them, and how to read documentation\n",
    "   * Do some basic troubleshooting, using google searches, documentation, and AI queries \n",
    "   * Learn the basics of data cleaning for text files\n",
    "   * Introduce some options for visualizing data through a matplotlib wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 1: Finding and opening .txt files**\n",
    "\n",
    "The best way to work with various files while programming is to ensure that file is in your *working directory*. Your working directory\n",
    "is just another way to say the folder you are currently coding in. VSCode always pushes you to be coding inside a folder. \n",
    "\n",
    "You have a file named *ascii-text-art.txt* that is located in your working directory, below is the code you need to \n",
    " open the file and read it using Python!\n",
    "\n",
    " You will need the path name for your file, right click the file and you have two options, both should work:\n",
    "   * copy path (full file path)\n",
    "   * copy relative path (relative, or shortened, file path)\n",
    "\n",
    "Below we are using three functions:\n",
    "   * open() -- built-in Python function, opens a file\n",
    "   * read() -- built-in Python function, reads a file\n",
    "   * print() -- built-in Python function, prints below the cell for user convenience \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ASCII File\n",
    "\n",
    "#r stands for \"read\"\n",
    "path = r\"YOUR PATH HERE\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are ever unsure where exactly you are coding, you can use the os module getcwd() function to show you!\n",
    "gwc means \"get current working directory\" -- https://www.w3schools.com/python/ref_os_getcwd.asp\n",
    "\n",
    "It returns the file path of wherever you are coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 2: Doing some basic text cleaning**\n",
    "\n",
    "We're now going to practice some basic text cleaning using a new .txt file. For this one, you need to download it yourself\n",
    "and add it to your working directory. You can find the file here: https://drive.google.com/file/d/1Xoenw8hs84nBpkcETqJOC_MBkopEjamS/view?usp=sharing\n",
    "\n",
    "First, you need to open the file, and then we'll get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"bush_2002_sotu.txt\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    bush_file = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be looking at the State of the Union (SOTU) address by Pres. George W. Bush from 2002. We've got one basic research question: \n",
    "*what are the most commonly referenced topics in Pres. Bush's SOTU address?* \n",
    "\n",
    "In order for us to begin to answer that question, we have to clean our dataset.\n",
    "\n",
    "The most common thing we need to do is remove *stopwords*. These are words that are so commonly used that they are totally irrelevant for \n",
    "textual analysis and waste processing time to evaluate. You can create your own custom list of stopwords, aka words that you want to ignore in your dataset, but\n",
    "most Python libraries for text analysis also include a pre-set list of stopwords.  \n",
    "\n",
    "Original SOTU file from: https://georgewbush-whitehouse.archives.gov/news/releases/2002/01/20020129-11.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of what stopwords are\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "stops = stopwords.words(\"english\")\n",
    "\n",
    "stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have this list of stopwords in the ntlk library, we are able to easily clean our data\n",
    "using a series of methods from the ntlk library, some built-in Python functions, and a for loop.\n",
    "\n",
    "Below are the functions/methods that we use:\n",
    "   *  set() -- turns an item into a set. This is not a strictly necessary step, but it converts a list into a set, which is\n",
    "       much faster to use than a list due to the way they're built in Python. \n",
    "   *  word_tokenize() -- this splits our dataset into tokens, aka smaller pieces. Tokenization is commonly used to make raw text\n",
    "       into something a programming language can actually use. This breaks the sentences into a list of individual words & punctuation.\n",
    "       You can see the results below on line 17 when we print the word_tokens variable\n",
    "   *  lower() -- converts everything into lowercase, this is very common when handling strings as upper and lower case letters are\n",
    "       fundamentally different in Python\n",
    "   *  isalpha() -- checks to see if a string is alphabetical or not, returns a Boolean (False or True)\n",
    "   *  append() -- this is used to append something to a list\n",
    "   *  print() -- prints off code of our choosing; an extremely helpful tool for debugging but does nothing to the actual program\n",
    "\n",
    "We will walk through some of these functions together and learn more about how to use them and understand what mandatory parameters they require. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning speech of stop words & punctuation\n",
    "#adapted code from https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "#optional step, converting into a set for faster processing\n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(bush_file)\n",
    "cleaned_speech = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w.lower() not in stop_words and w.isalpha():\n",
    "        cleaned_speech.append(w.lower())\n",
    " \n",
    "print(word_tokens)\n",
    "print(cleaned_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3: Building a WordCloud**\n",
    "\n",
    "We can then incorporate the WordCloud using the wordcloud library and matplotlib library.\n",
    "\n",
    "But uh-oh! The wordcloud library is depricated! This is a really common problem you can face when using libraries, and \n",
    "it's important to understand how to troubleshoot it. We'll walk through a few options together. \n",
    "\n",
    "Documentation:\n",
    "   * Wordcloud: https://amueller.github.io/word_cloud/\n",
    "   * matplotlib: https://matplotlib.org/\n",
    "\n",
    "\n",
    "\n",
    "Once we've figured that out, our next goal will be for you to try and figure out on your own how we could\n",
    "possibly save the wordcloud as a .png or .jpg file. \n",
    "\n",
    "Lastly, we only did very minimal data cleaning, so our wordcloud can be significantly improved. What are some limitations that you can spot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "# !pip uninstall pillow -y\n",
    "# !pip install pillow==9.5.0\n",
    "\n",
    "text_to_plot = \" \".join(cleaned_speech)\n",
    "\n",
    "# create a WordCloud \n",
    "wordcloud = WordCloud(width=1800, height=1500, \n",
    "                      background_color=\"white\", \n",
    "                      min_font_size=10).generate(text_to_plot)\n",
    "\n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize = (5,5), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "#your code -- how can we save a wordcloud as an image using Python? \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 4: Now you try!**\n",
    "\n",
    "Your goal is to adapt the code above to create your own wordcloud. You will need to:\n",
    "  * find a .txt file online (or perhaps you already have one you can use)\n",
    "  * bring that .txt file into your working directory\n",
    "  * do some very basic data cleaning\n",
    "  * put it into a wordcloud!\n",
    "\n",
    "If you finish with those steps, try reading the documentation for how you can mask a wordcloud\n",
    "with an image and see if you can get your code working!: https://amueller.github.io/word_cloud/auto_examples/masked.html#sphx-glr-auto-examples-masked-py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
