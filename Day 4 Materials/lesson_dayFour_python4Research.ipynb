{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rZknyZcPZp-"
      },
      "source": [
        "# Python for Researchers\n",
        "\n",
        "## Day Four Objectives\n",
        "\n",
        "*   learn and use Google Colab for writing Python\n",
        "*   practice a text analysis workflow using data pulled from the Journal of Language in Society\n",
        "*   review Pandas and dataframes\n",
        "*   review cleaning and working with textual data\n",
        "*   learn TF-IDF for preliminary analysis of a corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbVfzemAVpPf"
      },
      "source": [
        "## Section One: Our Data\n",
        "\n",
        "The journal Language in Society is an international journal in sociolinguistics. Our dataset contains the metadata of journal articles, reviews and other matter from 1980-2018. The data was pulled directly from JSTOR using Constellate Ithaka's dataset and analysis tool. \n",
        "\n",
        "We are using this data set because: it has already been gathered, cleaned and tested and it came from a reliable source.\n",
        "\n",
        "[Pull the dataset from here](https://drive.google.com/file/d/1qOBGbm7f8g3Hkb4deOAJmXkXrh2b7D37/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a dataframe "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0JJgHrjJ_1S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AWefZE6XF5B",
        "outputId": "79b8d200-e6ce-4284-f258-6365853f2113"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7_T5qjaXp3p"
      },
      "source": [
        "## Section Two: Cleaning Our Data\n",
        "\n",
        "First we'll drop everything we don't need and focus only on the columns that have relevant information. \n",
        "\n",
        "Let's say we want to explore our 'titles' column and look at what words and topics appear in titles the most over time. \n",
        "\n",
        "First, we'll drop the columns that don't have any information in them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iw-z1VHF5u8a",
        "outputId": "d349d007-3f68-4dc6-c407-cb914dc02861"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF Analysis\n",
        "\n",
        "TF-IDF stands for Text Frequency-Inverse Document Frequency \n",
        "\n",
        "TF- is basically a normalized word count, so total number of times a word appears in a doc / number of words in that doc \n",
        "\n",
        "TF-IDF - is the significance of a words normalized across an entire corpus, so a words 'importance' relative to how frequently it appears in an article and the corpus as whole (words that show up in many articles are less important -those that show up in fewer articles are more important)\n",
        "\n",
        "A common example is to think about how you might choose a place to eat in a new city. Since you are in a new place, not only do you want to go to a *good* place, you want to do to a place that is *unique*. Local reviews and reccomendations might point you towards what is good, but you'd need an additional tool or step to find out, among those places, which offers unique cuisine. \n",
        "\n",
        "In text analysis, some words might appear an entire set of works. For example, in almost every Steven King novel someone is wearing a blue chambray shirt. 'Chambray' would not be a *unique* word in King's works. However, if you were analyzing a corpus that had mulitple horror authors in it, chambray would (probably) show up as a word that is *uniquely* King's. \n",
        "\n",
        "We are going to analyze titles from our metdata and see if we can pull any unique words (this is not necessariyl ideal, we'd want to work with a much larger corpus, but this will give an idea of the workflow). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Iw16WzFBGIvo",
        "outputId": "dc2ef13c-e406-4833-efb4-59f7c081e9ad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn \n",
        "\n",
        "Go back and only select articles that are research articles-not reviews, and do the analysis again, what changes? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, use your Pandas knowledge and knowledge of comparators (==, !=, etc) to create code that will drop all rows that do not contain 'research-article' in the 'docSubType' category. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, you'll need to only select the columns you need to do the analysis just like we did before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now, run our four blocks of code to create our TF-IDF dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's show the most frequent words by year-just like we did before. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LDA \n",
        "\n",
        "Our next type of text analysis is LDA or Latent Dirlicht Allocation. LDA is used for topic modeling, or, for taking a set of texts and generating (based on probability) common topics in that set of data. LDA will also then show which topics are most associated with which document in the given dataset. \n",
        "\n",
        "How? First LDA passes all of the text into a bag of words, meaning order and context are ignored, only the words counts matter and how often words appear in a document with other words matter. \n",
        "\n",
        "For example, if we were working with movie reviews and generating topics from those reviews we might expect to see topics that look like: \n",
        "* ghost\n",
        "* house\n",
        "* woods\n",
        "\n",
        "* new york\n",
        "* love\n",
        "* romance\n",
        "\n",
        "* puppy\n",
        "* couple\n",
        "* children\n",
        "\n",
        "These groups of topics would apply to multiple reviews and show common themes and genres of film represented in the dataset. \n",
        "\n",
        "For our data, we are interested in seeinf if our titles hold any prevelant information about what kind of topics scholars submitting to the journal were discussing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZR-hRRGes9s",
        "outputId": "f160354c-f212-4270-d5a2-dda140f39afd"
      },
      "outputs": [],
      "source": [
        "pip install pyldavis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6WCfW6tZ0Xp"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "#nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Import the LDA items from scikit-learn\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import *\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bowp8jpZeoDq",
        "outputId": "9fbaca05-5edc-4c8b-f6fa-a997762a975b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### pyLDAvis\n",
        "\n",
        "There is a very nice visualization for LDA called pyLDAvis, but it can be a little deceiving to interpret. \n",
        "\n",
        "The blue bars represent saliency, or the frequency of terms in the corpus \n",
        "\n",
        "If you hover over one of the circles, the bars will change. You'll see a new set of words and new red bars. Here, the blue bars still show the term's frequency, but now *saliency* means a little more because these terms are not only frequent, but the ones that were found most relevant to one another in terms of defining the topic. \n",
        "\n",
        "In other words, the top words shown in the topic are the words the model grabbed onto the most to make that topic. The red bar shows the frequency of that word within the given topic. \n",
        "\n",
        "The circles are also showing us some information. The size of circles shows how many articles (or titles) in our case fit into that topic. The distribution of circles shows topics relative relationship to one another. Circles that are closer together are more related than circles that are further apart. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your turn \n",
        "\n",
        "[Using the following dataset practice doing LDA.](https://drive.google.com/file/d/1gwn7x2ZDLOnwVrJp_WzQGQBLACRbpX0V/view?usp=sharing)\n",
        "\n",
        "The data represents the full text of the articles pulled from the journal Language in Society, although it's been broken down into unigrams and then spliced back together in order to get accurate word counts for articles. \n",
        "\n",
        "Use the 'text' column to perform the analysis in the same way we did with our 'title' column"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
